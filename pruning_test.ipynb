{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from genericpath import isfile\n",
    "import json\n",
    "import os\n",
    "if __name__ == '__main__':\n",
    "    #os.sys.path.append('./pytorch_geometric/torch_geometric')\n",
    "    os.sys.path.append('./src')\n",
    "from src.model.model import MMGNet\n",
    "from src.utils.config import Config\n",
    "from utils import util\n",
    "import torch\n",
    "import argparse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_config():\n",
    "    # load config file\n",
    "    config = Config(\"./config/mmgnet.json\")\n",
    "    #print(config)\n",
    "    if 'NAME' not in config:\n",
    "        config_name = os.path.basename('./config/mmgnet.json')\n",
    "        if len(config_name) > len('config_'):\n",
    "            name = config_name[len('config_'):]\n",
    "            name = os.path.splitext(name)[0]\n",
    "            translation_table = dict.fromkeys(map(ord, '!@#$'), None)\n",
    "            name = name.translate(translation_table)\n",
    "            config['NAME'] = name            \n",
    "    config.LOADBEST = ''\n",
    "    config.MODE = 'train'\n",
    "    config.exp = 'test'\n",
    "\n",
    "    return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 160 classes ===\n",
      "| 0             armchair:0.006|| 1             backpack:0.019|\n",
      "| 2                  bag:0.005|| 3                 ball:0.143|\n",
      "| 4                  bar:0.071|| 5                basin:0.286|\n",
      "| 6               basket:0.009|| 7         bath cabinet:0.016|\n",
      "| 8              bathtub:0.036|| 9                  bed:0.009|\n",
      "|10        bedside table:0.061||11                bench:0.011|\n",
      "|12                bidet:0.200||13                  bin:0.037|\n",
      "|14              blanket:0.007||15               blinds:0.014|\n",
      "|16                board:0.087||17                 book:0.050|\n",
      "|18                books:0.080||19            bookshelf:0.065|\n",
      "|20               bottle:0.041||21                  box:0.001|\n",
      "|22                bread:0.286||23               bucket:0.015|\n",
      "|24              cabinet:0.004||25               carpet:0.061|\n",
      "|26              ceiling:0.002||27                chair:0.001|\n",
      "|28             cleanser:0.500||29                clock:0.059|\n",
      "|30               closet:0.083||31              clothes:0.006|\n",
      "|32        clothes dryer:0.030||33              clutter:0.016|\n",
      "|34       coffee machine:0.080||35         coffee table:0.015|\n",
      "|36              commode:0.008||37        computer desk:0.250|\n",
      "|38                couch:0.025||39          couch table:0.040|\n",
      "|40              counter:0.021||41                  cup:0.286|\n",
      "|42             cupboard:0.038||43              curtain:0.003|\n",
      "|44              cushion:0.011||45        cutting board:0.080|\n",
      "|46           decoration:0.022||47                 desk:0.008|\n",
      "|48         dining chair:0.038||49         dining table:0.050|\n",
      "|50                 door:0.003||51            doorframe:0.006|\n",
      "|52               drawer:0.059||53                 drum:0.667|\n",
      "|54       drying machine:0.154||55        extractor fan:0.250|\n",
      "|56            fireplace:0.105||57                floor:0.001|\n",
      "|58               flower:0.057||59              flowers:0.077|\n",
      "|60               folder:0.200||61                 food:0.182|\n",
      "|62            footstool:0.400||63                frame:0.026|\n",
      "|64          fruit plate:1.000||65              garbage:0.054|\n",
      "|66          garbage bin:0.087||67                grass:0.500|\n",
      "|68           hand dryer:0.133||69               heater:0.007|\n",
      "|70                 item:0.003||71               jacket:1.000|\n",
      "|72                  jar:0.125||73               kettle:0.047|\n",
      "|74    kitchen appliance:0.024||75      kitchen cabinet:0.006|\n",
      "|76      kitchen counter:0.016||77         kitchen hood:0.222|\n",
      "|78               ladder:0.111||79                 lamp:0.003|\n",
      "|80               laptop:0.036||81       laundry basket:0.059|\n",
      "|82                light:0.006||83              machine:0.250|\n",
      "|84        magazine rack:0.667||85                 menu:1.000|\n",
      "|86            microwave:0.030||87               mirror:0.018|\n",
      "|88              monitor:0.009||89              napkins:0.143|\n",
      "|90           nightstand:0.013||91               object:0.004|\n",
      "|92              objects:0.105||93            organizer:0.056|\n",
      "|94              ottoman:0.048||95                 oven:0.028|\n",
      "|96                 pack:0.059||97                  pan:0.167|\n",
      "|98          paper towel:0.091||99               papers:0.286|\n",
      "|100                   pc:0.022||101              picture:0.005|\n",
      "|102        pile of books:0.154||103       pile of papers:0.400|\n",
      "|104               pillow:0.001||105                 pipe:0.047|\n",
      "|106                plant:0.003||107                plate:0.041|\n",
      "|108               player:0.250||109                  pot:0.054|\n",
      "|110              printer:0.047||111                 rack:0.020|\n",
      "|112             radiator:0.014||113          recycle bin:0.111|\n",
      "|114         refrigerator:0.027||115        rocking chair:0.667|\n",
      "|116                scale:0.087||117               screen:0.667|\n",
      "|118                shelf:0.002||119                 shoe:0.400|\n",
      "|120            shoe rack:0.400||121                shoes:0.024|\n",
      "|122             showcase:0.034||123               shower:0.043|\n",
      "|124       shower curtain:0.044||125         shower floor:0.200|\n",
      "|126          shower wall:0.077||127           side table:0.033|\n",
      "|128                 sink:0.006||129            soap dish:0.500|\n",
      "|130               socket:0.286||131                 sofa:0.010|\n",
      "|132           sofa chair:0.083||133                stair:0.143|\n",
      "|134                stand:0.017||135                stool:0.007|\n",
      "|136                stove:0.024||137       stuffed animal:0.069|\n",
      "|138             suitcase:0.036||139                table:0.003|\n",
      "|140           table lamp:0.077||141            telephone:0.042|\n",
      "|142              toaster:0.250||143               toilet:0.016|\n",
      "|144         toilet brush:0.074||145         toilet paper:0.043|\n",
      "|146 toilet paper dispenser:0.250||147                towel:0.007|\n",
      "|148            trash can:0.007||149             trashcan:0.333|\n",
      "|150                 tube:0.095||151                   tv:0.012|\n",
      "|152             tv stand:0.026||153                 vase:0.035|\n",
      "|154                 wall:0.000||155             wardrobe:0.009|\n",
      "|156      washing machine:0.027||157       washing powder:0.250|\n",
      "|158               window:0.003||159           windowsill:0.009|\n",
      "\n",
      "=== 26 relationships ===\n",
      "| 0         supported by 0.001|| 1                 left 0.000|\n",
      "| 2                right 0.000|| 3                front 0.000|\n",
      "| 4               behind 0.000|| 5             close by 0.000|\n",
      "| 6               inside 1.000|| 7          bigger than 0.001|\n",
      "| 8         smaller than 0.001|| 9          higher than 0.001|\n",
      "|10           lower than 0.001||11     same symmetry as 0.004|\n",
      "|12              same as 0.000||13          attached to 0.000|\n",
      "|14          standing on 0.000||15             lying on 0.000|\n",
      "|16           hanging on 0.001||17         connected to 0.005|\n",
      "|18      leaning against 0.005||19              part of 0.015|\n",
      "|20         belonging to 0.006||21             build in 0.004|\n",
      "|22          standing in 0.004||23                cover 0.022|\n",
      "|24             lying in 0.010||25           hanging in 0.100|\n",
      "\n",
      "num of data: 3845\n",
      "/home/knuvi/hojun/CVPR2023-VLSAT/data/3RScan/f62fd5fd-9a3f-2f44-883a-1e5cf819608e\n",
      "=== 160 classes ===\n",
      "| 0             armchair:0.045|| 1             backpack:0.400|\n",
      "| 2                  bag:0.056|| 3                 ball:0.400|\n",
      "| 4                  bar:0.500|| 5                basin:0.500|\n",
      "| 6               basket:0.182|| 7         bath cabinet:0.080|\n",
      "| 8              bathtub:0.400|| 9                  bed:0.286|\n",
      "|10        bedside table:0.400||11                bench:0.105|\n",
      "|12                bidet:0.667||13                  bin:0.667|\n",
      "|14              blanket:0.044||15               blinds:0.400|\n",
      "|16                board:0.333||17                 book:0.182|\n",
      "|18                books:0.333||19            bookshelf:0.250|\n",
      "|20               bottle:0.500||21                  box:0.019|\n",
      "|22                bread:1.000||23               bucket:0.250|\n",
      "|24              cabinet:0.016||25               carpet:0.250|\n",
      "|26              ceiling:0.012||27                chair:0.010|\n",
      "|28             cleanser:0.400||29                clock:0.095|\n",
      "|30               closet:0.667||31              clothes:0.095|\n",
      "|32        clothes dryer:0.667||33              clutter:0.061|\n",
      "|34       coffee machine:0.500||35         coffee table:0.069|\n",
      "|36              commode:0.062||37        computer desk:0.333|\n",
      "|38                couch:0.053||39          couch table:0.118|\n",
      "|40              counter:0.143||41                  cup:1.000|\n",
      "|42             cupboard:0.200||43              curtain:0.017|\n",
      "|44              cushion:0.033||45        cutting board:0.250|\n",
      "|46           decoration:0.056||47                 desk:0.100|\n",
      "|48         dining chair:0.182||49         dining table:0.286|\n",
      "|50                 door:0.018||51            doorframe:0.035|\n",
      "|52               drawer:0.333||53                 drum:0.333|\n",
      "|54       drying machine:0.200||55        extractor fan:0.500|\n",
      "|56            fireplace:0.154||57                floor:0.004|\n",
      "|58               flower:0.250||59              flowers:0.200|\n",
      "|60               folder:0.250||61                 food:0.333|\n",
      "|62            footstool:0.667||63                frame:0.118|\n",
      "|64          fruit plate:0.500||65              garbage:0.400|\n",
      "|66          garbage bin:0.286||67                grass:0.667|\n",
      "|68           hand dryer:0.286||69               heater:0.051|\n",
      "|70                 item:0.025||71               jacket:0.667|\n",
      "|72                  jar:0.250||73               kettle:0.250|\n",
      "|74    kitchen appliance:0.167||75      kitchen cabinet:0.036|\n",
      "|76      kitchen counter:0.105||77         kitchen hood:0.286|\n",
      "|78               ladder:0.667||79                 lamp:0.019|\n",
      "|80               laptop:0.500||81       laundry basket:0.182|\n",
      "|82                light:0.062||83              machine:0.500|\n",
      "|84        magazine rack:0.667||85                 menu:0.154|\n",
      "|86            microwave:0.182||87               mirror:0.222|\n",
      "|88              monitor:0.118||89              napkins:1.000|\n",
      "|90           nightstand:1.000||91               object:0.027|\n",
      "|92              objects:1.000||93            organizer:0.500|\n",
      "|94              ottoman:0.065||95                 oven:0.118|\n",
      "|96                 pack:0.133||97                  pan:0.667|\n",
      "|98          paper towel:0.500||99               papers:1.000|\n",
      "|100                   pc:0.500||101              picture:0.024|\n",
      "|102        pile of books:0.222||103       pile of papers:0.400|\n",
      "|104               pillow:0.010||105                 pipe:0.400|\n",
      "|106                plant:0.013||107                plate:0.182|\n",
      "|108               player:0.500||109                  pot:0.250|\n",
      "|110              printer:0.143||111                 rack:0.167|\n",
      "|112             radiator:0.100||113          recycle bin:1.000|\n",
      "|114         refrigerator:0.286||115        rocking chair:0.667|\n",
      "|116                scale:0.667||117               screen:0.333|\n",
      "|118                shelf:0.014||119                 shoe:0.400|\n",
      "|120            shoe rack:0.500||121                shoes:0.333|\n",
      "|122             showcase:0.167||123               shower:0.500|\n",
      "|124       shower curtain:0.250||125         shower floor:0.667|\n",
      "|126          shower wall:0.087||127           side table:0.083|\n",
      "|128                 sink:0.033||129            soap dish:0.667|\n",
      "|130               socket:1.000||131                 sofa:0.054|\n",
      "|132           sofa chair:0.400||133                stair:0.500|\n",
      "|134                stand:0.143||135                stool:0.087|\n",
      "|136                stove:0.111||137       stuffed animal:1.000|\n",
      "|138             suitcase:0.286||139                table:0.023|\n",
      "|140           table lamp:0.250||141            telephone:0.333|\n",
      "|142              toaster:0.400||143               toilet:0.071|\n",
      "|144         toilet brush:0.200||145         toilet paper:0.167|\n",
      "|146 toilet paper dispenser:0.182||147                towel:0.071|\n",
      "|148            trash can:0.059||149             trashcan:0.500|\n",
      "|150                 tube:0.500||151                   tv:0.051|\n",
      "|152             tv stand:0.062||153                 vase:0.053|\n",
      "|154                 wall:0.003||155             wardrobe:0.143|\n",
      "|156      washing machine:0.133||157       washing powder:1.000|\n",
      "|158               window:0.023||159           windowsill:0.118|\n",
      "\n",
      "=== 26 relationships ===\n",
      "| 0         supported by 0.004|| 1                 left 0.001|\n",
      "| 2                right 0.001|| 3                front 0.001|\n",
      "| 4               behind 0.001|| 5             close by 0.001|\n",
      "| 6               inside 1.000|| 7          bigger than 0.012|\n",
      "| 8         smaller than 0.012|| 9          higher than 0.005|\n",
      "|10           lower than 0.005||11     same symmetry as 0.020|\n",
      "|12              same as 0.005||13          attached to 0.001|\n",
      "|14          standing on 0.001||15             lying on 0.004|\n",
      "|16           hanging on 0.006||17         connected to 0.029|\n",
      "|18      leaning against 0.050||19              part of 0.143|\n",
      "|20         belonging to 0.031||21             build in 0.029|\n",
      "|22          standing in 0.038||23                cover 0.053|\n",
      "|24             lying in 0.027||25           hanging in 0.500|\n",
      "\n",
      "num of data: 548\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<src.model.model.MMGNet at 0x7fab997a3af0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = load_config()\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"0\"\n",
    "util.set_random_seed(config.SEED)\n",
    "\n",
    "# if config.VERBOSE:\n",
    "#     print(config)\n",
    "\n",
    "model = MMGNet(config)\n",
    "model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Mmgnet_teacher(\n",
       "  (obj_encoder): PointNetfeat(\n",
       "    (relu): ReLU()\n",
       "    (conv1): Conv1d(3, 64, kernel_size=(1,), stride=(1,))\n",
       "    (conv2): Conv1d(64, 128, kernel_size=(1,), stride=(1,))\n",
       "    (conv3): Conv1d(128, 768, kernel_size=(1,), stride=(1,))\n",
       "  )\n",
       "  (rel_encoder_2d): PointNetfeat(\n",
       "    (relu): ReLU()\n",
       "    (conv1): Conv1d(11, 64, kernel_size=(1,), stride=(1,))\n",
       "    (conv2): Conv1d(64, 128, kernel_size=(1,), stride=(1,))\n",
       "    (conv3): Conv1d(128, 512, kernel_size=(1,), stride=(1,))\n",
       "  )\n",
       "  (rel_encoder_3d): PointNetfeat(\n",
       "    (relu): ReLU()\n",
       "    (conv1): Conv1d(11, 64, kernel_size=(1,), stride=(1,))\n",
       "    (conv2): Conv1d(64, 128, kernel_size=(1,), stride=(1,))\n",
       "    (conv3): Conv1d(128, 512, kernel_size=(1,), stride=(1,))\n",
       "  )\n",
       "  (mmg): MMG(\n",
       "    (self_attn): ModuleList(\n",
       "      (0): MultiHeadAttention(\n",
       "        (attention): ScaledDotProductAttention(\n",
       "          (fc_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (fc_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (fc_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (fc_o): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (1): MultiHeadAttention(\n",
       "        (attention): ScaledDotProductAttention(\n",
       "          (fc_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (fc_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (fc_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (fc_o): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (cross_attn): ModuleList(\n",
       "      (0): MultiHeadAttention(\n",
       "        (attention): ScaledDotProductAttention(\n",
       "          (fc_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (fc_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (fc_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (fc_o): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (1): MultiHeadAttention(\n",
       "        (attention): ScaledDotProductAttention(\n",
       "          (fc_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (fc_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (fc_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (fc_o): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (cross_attn_rel): ModuleList(\n",
       "      (0): MultiHeadAttention(\n",
       "        (attention): ScaledDotProductAttention(\n",
       "          (fc_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (fc_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (fc_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (fc_o): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (1): MultiHeadAttention(\n",
       "        (attention): ScaledDotProductAttention(\n",
       "          (fc_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (fc_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (fc_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (fc_o): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (gcn_2ds): ModuleList(\n",
       "      (0): GraphEdgeAttenNetwork(\n",
       "        (index_get): Gen_Index()\n",
       "        (index_aggr): Aggre_Index()\n",
       "        (edgeatten): MultiHeadedEdgeAttention(\n",
       "          (nn_edge): Sequential(\n",
       "            (0): Linear(in_features=1536, out_features=1024, bias=True)\n",
       "            (1): ReLU()\n",
       "            (2): Linear(in_features=1024, out_features=512, bias=True)\n",
       "          )\n",
       "          (nn): mySequential(\n",
       "            (0): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "            (1): ReLU()\n",
       "            (2): Dropout(p=0.5, inplace=False)\n",
       "            (3): Conv1d(128, 32, kernel_size=(1,), stride=(1,))\n",
       "          )\n",
       "          (proj_edge): Sequential(\n",
       "            (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (proj_query): Sequential(\n",
       "            (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (proj_value): Sequential(\n",
       "            (0): Linear(in_features=512, out_features=256, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (prop): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=768, out_features=512, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (1): GraphEdgeAttenNetwork(\n",
       "        (index_get): Gen_Index()\n",
       "        (index_aggr): Aggre_Index()\n",
       "        (edgeatten): MultiHeadedEdgeAttention(\n",
       "          (nn_edge): Sequential(\n",
       "            (0): Linear(in_features=1536, out_features=1024, bias=True)\n",
       "            (1): ReLU()\n",
       "            (2): Linear(in_features=1024, out_features=512, bias=True)\n",
       "          )\n",
       "          (nn): mySequential(\n",
       "            (0): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "            (1): ReLU()\n",
       "            (2): Dropout(p=0.5, inplace=False)\n",
       "            (3): Conv1d(128, 32, kernel_size=(1,), stride=(1,))\n",
       "          )\n",
       "          (proj_edge): Sequential(\n",
       "            (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (proj_query): Sequential(\n",
       "            (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (proj_value): Sequential(\n",
       "            (0): Linear(in_features=512, out_features=256, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (prop): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=768, out_features=512, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (gcn_3ds): ModuleList(\n",
       "      (0): GraphEdgeAttenNetwork(\n",
       "        (index_get): Gen_Index()\n",
       "        (index_aggr): Aggre_Index()\n",
       "        (edgeatten): MultiHeadedEdgeAttention(\n",
       "          (nn_edge): Sequential(\n",
       "            (0): Linear(in_features=1536, out_features=1024, bias=True)\n",
       "            (1): ReLU()\n",
       "            (2): Linear(in_features=1024, out_features=512, bias=True)\n",
       "          )\n",
       "          (nn): mySequential(\n",
       "            (0): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "            (1): ReLU()\n",
       "            (2): Dropout(p=0.5, inplace=False)\n",
       "            (3): Conv1d(128, 32, kernel_size=(1,), stride=(1,))\n",
       "          )\n",
       "          (proj_edge): Sequential(\n",
       "            (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (proj_query): Sequential(\n",
       "            (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (proj_value): Sequential(\n",
       "            (0): Linear(in_features=512, out_features=256, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (prop): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=768, out_features=512, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (1): GraphEdgeAttenNetwork(\n",
       "        (index_get): Gen_Index()\n",
       "        (index_aggr): Aggre_Index()\n",
       "        (edgeatten): MultiHeadedEdgeAttention(\n",
       "          (nn_edge): Sequential(\n",
       "            (0): Linear(in_features=1536, out_features=1024, bias=True)\n",
       "            (1): ReLU()\n",
       "            (2): Linear(in_features=1024, out_features=512, bias=True)\n",
       "          )\n",
       "          (nn): mySequential(\n",
       "            (0): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "            (1): ReLU()\n",
       "            (2): Dropout(p=0.5, inplace=False)\n",
       "            (3): Conv1d(128, 32, kernel_size=(1,), stride=(1,))\n",
       "          )\n",
       "          (proj_edge): Sequential(\n",
       "            (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (proj_query): Sequential(\n",
       "            (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (proj_value): Sequential(\n",
       "            (0): Linear(in_features=512, out_features=256, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (prop): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=768, out_features=512, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (self_attn_fc): Sequential(\n",
       "      (0): Linear(in_features=4, out_features=32, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "      (3): Linear(in_features=32, out_features=32, bias=True)\n",
       "      (4): ReLU()\n",
       "      (5): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "      (6): Linear(in_features=32, out_features=8, bias=True)\n",
       "    )\n",
       "    (drop_out): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       "  (triplet_projector_3d): Sequential(\n",
       "    (0): Linear(in_features=1536, out_features=1024, bias=True)\n",
       "    (1): Dropout(p=0.5, inplace=False)\n",
       "    (2): ReLU()\n",
       "    (3): Linear(in_features=1024, out_features=512, bias=True)\n",
       "  )\n",
       "  (triplet_projector_2d): Sequential(\n",
       "    (0): Linear(in_features=1536, out_features=1024, bias=True)\n",
       "    (1): Dropout(p=0.5, inplace=False)\n",
       "    (2): ReLU()\n",
       "    (3): Linear(in_features=1024, out_features=512, bias=True)\n",
       "  )\n",
       "  (clip_adapter): AdapterModel(\n",
       "    (fc1): Linear(in_features=512, out_features=256, bias=True)\n",
       "    (relu): ReLU()\n",
       "    (fc2): Linear(in_features=256, out_features=512, bias=True)\n",
       "  )\n",
       "  (mlp_3d): Sequential(\n",
       "    (0): Linear(in_features=768, out_features=504, bias=True)\n",
       "    (1): BatchNorm1d(504, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (rel_predictor_3d): PointNetRelClsMulti(\n",
       "    (fc1): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
       "    (fc3): Linear(in_features=256, out_features=26, bias=True)\n",
       "    (dropout): Dropout(p=0.3, inplace=False)\n",
       "    (relu): ReLU()\n",
       "  )\n",
       "  (rel_predictor_2d): PointNetRelClsMulti(\n",
       "    (fc1): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
       "    (fc3): Linear(in_features=256, out_features=26, bias=True)\n",
       "    (dropout): Dropout(p=0.3, inplace=False)\n",
       "    (relu): ReLU()\n",
       "  )\n",
       "  (clip_model): CLIP(\n",
       "    (visual): VisionTransformer(\n",
       "      (conv1): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32), bias=False)\n",
       "      (ln_pre): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (transformer): Transformer(\n",
       "        (resblocks): Sequential(\n",
       "          (0): ResidualAttentionBlock(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Sequential(\n",
       "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (gelu): QuickGELU()\n",
       "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (1): ResidualAttentionBlock(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Sequential(\n",
       "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (gelu): QuickGELU()\n",
       "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (2): ResidualAttentionBlock(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Sequential(\n",
       "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (gelu): QuickGELU()\n",
       "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (3): ResidualAttentionBlock(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Sequential(\n",
       "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (gelu): QuickGELU()\n",
       "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (4): ResidualAttentionBlock(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Sequential(\n",
       "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (gelu): QuickGELU()\n",
       "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (5): ResidualAttentionBlock(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Sequential(\n",
       "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (gelu): QuickGELU()\n",
       "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (6): ResidualAttentionBlock(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Sequential(\n",
       "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (gelu): QuickGELU()\n",
       "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (7): ResidualAttentionBlock(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Sequential(\n",
       "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (gelu): QuickGELU()\n",
       "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (8): ResidualAttentionBlock(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Sequential(\n",
       "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (gelu): QuickGELU()\n",
       "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (9): ResidualAttentionBlock(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Sequential(\n",
       "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (gelu): QuickGELU()\n",
       "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (10): ResidualAttentionBlock(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Sequential(\n",
       "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (gelu): QuickGELU()\n",
       "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (11): ResidualAttentionBlock(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Sequential(\n",
       "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (gelu): QuickGELU()\n",
       "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (ln_post): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (resblocks): Sequential(\n",
       "        (0): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (1): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (2): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (3): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (4): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (5): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (6): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (7): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (8): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (9): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (10): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (11): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (token_embedding): Embedding(49408, 512)\n",
       "    (ln_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (obj_predictor_2d): Linear(in_features=512, out_features=160, bias=True)\n",
       "  (obj_predictor_3d): Linear(in_features=512, out_features=160, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModuleList(\n",
       "  (0): MultiHeadAttention(\n",
       "    (attention): ScaledDotProductAttention(\n",
       "      (fc_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (fc_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (fc_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (fc_o): Linear(in_features=512, out_features=512, bias=True)\n",
       "    )\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (1): MultiHeadAttention(\n",
       "    (attention): ScaledDotProductAttention(\n",
       "      (fc_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (fc_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (fc_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (fc_o): Linear(in_features=512, out_features=512, bias=True)\n",
       "    )\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model.mmg.self_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GraphEdgeAttenNetwork(\n",
       "  (index_get): Gen_Index()\n",
       "  (index_aggr): Aggre_Index()\n",
       "  (edgeatten): MultiHeadedEdgeAttention(\n",
       "    (nn_edge): Sequential(\n",
       "      (0): Linear(in_features=1536, out_features=1024, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=1024, out_features=512, bias=True)\n",
       "    )\n",
       "    (nn): mySequential(\n",
       "      (0): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "      (1): ReLU()\n",
       "      (2): Dropout(p=0.5, inplace=False)\n",
       "      (3): Conv1d(128, 32, kernel_size=(1,), stride=(1,))\n",
       "    )\n",
       "    (proj_edge): Sequential(\n",
       "      (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "    )\n",
       "    (proj_query): Sequential(\n",
       "      (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "    )\n",
       "    (proj_value): Sequential(\n",
       "      (0): Linear(in_features=512, out_features=256, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (prop): Sequential(\n",
       "    (0): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=768, out_features=512, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model.mmg.self_attn[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=512, out_features=512, bias=True)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model.mmg.self_attn[0].attention.fc_o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch_pruning as tp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Head #0\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MMG_student' object has no attribute 'head_dim'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mmmg\u001b[38;5;241m.\u001b[39mmodules():\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHead #\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m%\u001b[39mhead_id)\n\u001b[0;32m----> 4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[Before Pruning] Num Heads: \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m, Head Dim: \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m =>\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m%\u001b[39m(m\u001b[38;5;241m.\u001b[39mnum_heads, \u001b[43mm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhead_dim\u001b[49m))\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m()\n\u001b[1;32m      6\u001b[0m     head_id\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/vlsat/lib/python3.8/site-packages/torch/nn/modules/module.py:1207\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1205\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1206\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1207\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   1208\u001b[0m     \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, name))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'MMG_student' object has no attribute 'head_dim'"
     ]
    }
   ],
   "source": [
    "head_id = 0\n",
    "for m in model.model.mmg.modules():\n",
    "    print(\"Head #%d\"%head_id)\n",
    "    print(\"[Before Pruning] Num Heads: %d, Head Dim: %d =>\"%(m.num_heads, m.head_dim))\n",
    "    print()\n",
    "    head_id+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vlsat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
